How to automatically set the 'sigma' parameter for Cedar-detect? Idea: acquire a
test image, use Cedar-detect with a default sigma value to find stars, and run
Tetra3 to plate solve it.

Next, using the same test image, run Cedar-detect using a sweep of sigma values.
For each sigma value evaluate the star candidates as follows:
* Using the plate solution, convert the candidate's x/y to ra/dec and look
  for a sufficiently nearby star in a suitably magnitude-limited catalogue.
* Tally up the number of star candidates S that correspond to a catalogue star
  vs the number of star candidates N that do not match the catalogue.
* F = N / (S + N) is the fraction of Cedar-detect candidates that are spurious
  (noise-induced) detections.

When sigma is high, F (fraction of spurious candidates) will be low. As sigma
is lowered, S + N will increase (overall more Cedar-detect candidates) but the
fraction F of bad candidates will also rise.

We can define a maximum tolerable F value (say 0.01?) and use this to determine
the minimum sigma value to be used when the slider is towards "speed". When the
slider is moved toward "quality" we can raise the sigma value along with the
exposure times).


Feedback in focus mode: we display the number of detected stars using a slider
for a bar graph. Improvements to consider:

* Over a window of the most recent N frames, determine which stars are detected
  in (nearly) all of the frames (based on x/y proximity between frames). Display
  the count of these "robustly detected" stars. This count will not include stars
  at the edge of detectability nor false positives (noise).


converting raw without debayering:
dcraw -D -T -W -b 256 -g 1 1 *.dng
